{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def llm(input_text, stop=[\"\\n\"]):\n",
    "    url = \"http://47.88.8.18:8088/api/ask\"\n",
    "    HTTP_LLM_API_KEY='eyJ0eXAiOiJqd3QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VybmFtZSI6IjM5NDc3MyIsInBhc3N3b3JkIjoiMzk0NzczMTIzIiwiZXhwIjoyMDIxNjE4MzE3fQ.oQx2Rh-GJ_C29AfHTHE4x_2kVyy7NamwQRKRA4GPA94'\n",
    "    headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Authorization\": \"Bearer \" + HTTP_LLM_API_KEY\n",
    "                }\n",
    "    data = {\n",
    "            \"model\": 'gpt-4',\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": input_text}\n",
    "            ],\n",
    "            \"n\": 1,\n",
    "            \"temperature\": 0.0,\n",
    "            # \"stop\": [\"\\n\"]\n",
    "            }\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    response = response.json()\n",
    "    new_response = response['data']['response']\n",
    "    return new_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def llm_equal(question, gt, pred):\n",
    "    prompt = f\"I have a ground truth answer and a suspect answer for a question. I need to determine if the suspect answer is correct by comparing it to the ground truth answer. Please compare the two answers and let me know if the suspect answer is correct. Please also provide the reason behind your comparison.\\nQuestion:{question}\\nGround Truth Answer: {gt}\\nSuspect Answer: {pred}\\nYou need respond in the following strcure.\\n\\nCorrect:[True or False]\\nReason:\"\n",
    "    return llm(prompt.format(question=question,gt=gt, pred=pred))\n",
    "\n",
    "def parse_output(text):\n",
    "    # Initialize the dictionary\n",
    "    result_dict = {\"correct\": None, \"reason\": None}\n",
    "    \n",
    "    # Split the output text into lines\n",
    "    if '\\n' not in text:\n",
    "        result_dict['correct']=False\n",
    "        result_dict['reason']=None\n",
    "\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Extract the \"Correct\" and \"Reason\" parts\n",
    "    for line in lines:\n",
    "        if line.startswith('Correct:'):\n",
    "            correct_value = line[len('Correct: '):].strip()\n",
    "            # Convert 'True' and 'False' to boolean values\n",
    "            if correct_value == 'True':\n",
    "                result_dict[\"correct\"] = True\n",
    "            elif correct_value == 'False':\n",
    "                result_dict[\"correct\"] = False\n",
    "        elif line.startswith('Reason:'):\n",
    "            result_dict[\"reason\"] = line[len('Reason: '):].strip()\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "\n",
    "def normalize_answer(s):\n",
    "  def remove_articles(text):\n",
    "    return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "  \n",
    "  def white_space_fix(text):\n",
    "      return \" \".join(text.split())\n",
    "\n",
    "  def remove_punc(text):\n",
    "      exclude = set(string.punctuation)\n",
    "      return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "  def lower(text):\n",
    "      return text.lower()\n",
    "\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "  normalized_prediction = normalize_answer(prediction)\n",
    "  normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "  ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "  if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "    return ZERO_METRIC\n",
    "  if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "    return ZERO_METRIC\n",
    "  \n",
    "  prediction_tokens = normalized_prediction.split()\n",
    "  ground_truth_tokens = normalized_ground_truth.split()\n",
    "  common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "  num_same = sum(common.values())\n",
    "  if num_same == 0:\n",
    "    return ZERO_METRIC\n",
    "  precision = 1.0 * num_same / len(prediction_tokens)\n",
    "  recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "  f1 = (2 * precision * recall) / (precision + recall)\n",
    "  return f1, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hotpotqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "json_file_path = 'outputs/hotpotqa/qwen2-57_hotpotqa_output.json'\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "data_hotpotqa = [item for item in data if 'cot_answer' in item]\n",
    "data_len = len(data_hotpotqa)\n",
    "print(data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What number largest commercial airline in the United States is Hana Hou! published for?',\n",
       " 'answer': '8th',\n",
       " 'id': 1,\n",
       " 's1_domains': ['factual'],\n",
       " 'cot_response': 'First, Hana Hou! is the in-flight magazine of Hawaiian Airlines. Second, Hawaiian Airlines is the eighth-largest commercial airline in the United States. The answer is eight.',\n",
       " 'cot_answer': 'eight.',\n",
       " 'cot_sc_score': 1.0,\n",
       " 'cot_sc_response': 'First, Hana Hou! is the in-flight magazine of Hawaiian Airlines. Second, Hawaiian Airlines is the eighth-largest commercial airline in the United States. The answer is eight.',\n",
       " 'cot_sc_answer': 'eight.',\n",
       " 'cot_sc_rationales': ['Hana Hou! is the in-flight magazine of Hawaiian Airlines.',\n",
       "  'Hawaiian Airlines is the eighth-largest commercial airline in the United States.']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hotpotqa[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Question\", \"True answer\", \"CoT answer\", \"CoT-SC answer\", \"CoT-SC score\", \"CoK answer\"]\n",
    "df_hotpotqa = pd.DataFrame(columns = columns)\n",
    "for i in range(data_len):\n",
    "    df_hotpotqa.at[i,\"Question\"] = data_hotpotqa[i]['question']\n",
    "    df_hotpotqa.at[i,\"True answer\"] = data_hotpotqa[i]['answer']\n",
    "    df_hotpotqa.at[i,\"CoT answer\"] = data_hotpotqa[i]['cot_answer'].rstrip('.')\n",
    "    df_hotpotqa.at[i,\"CoT-SC answer\"] = data_hotpotqa[i]['cot_sc_answer'].rstrip('.')\n",
    "    df_hotpotqa.at[i,\"CoT-SC score\"] = data_hotpotqa[i]['cot_sc_score']\n",
    "    df_hotpotqa.at[i,\"CoK answer\"] = data_hotpotqa[i]['final_answer'].rstrip('.') if 'final_answer' in data_hotpotqa[i].keys() else data_hotpotqa[i]['cot_sc_answer'].rstrip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "em:0.23\n",
      "f1:0.33064502164502163\n"
     ]
    }
   ],
   "source": [
    "ems = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(data_len):\n",
    "    gt = normalize_answer(df_hotpotqa.at[i,\"True answer\"])\n",
    "    pred = normalize_answer(df_hotpotqa.at[i,\"CoK answer\"])\n",
    "    em = 1 if pred==gt else 0\n",
    "    f1, _, _ = f1_score(pred, gt)\n",
    "    ems.append(em)\n",
    "    f1s.append(f1)\n",
    "\n",
    "print(f\"em:{sum(ems)/len(ems)}\")\n",
    "print(f\"f1:{sum(f1s)/len(f1s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "llm_eval = []\n",
    "for i in range(data_len):\n",
    "    print(i)\n",
    "    gt = normalize_answer(df_hotpotqa.at[i,\"True answer\"])\n",
    "    pred = normalize_answer(df_hotpotqa.at[i,\"CoK answer\"])\n",
    "    llm_eval.append(llm_equal(df_hotpotqa.at[i,\"Question\"], gt, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm true:0.5526315789473685\n"
     ]
    }
   ],
   "source": [
    "llm_evals=[]\n",
    "for item in llm_eval:\n",
    "    llm_evals.append(parse_output(item))\n",
    "    \n",
    "trues = [item['correct'] for item in llm_evals]\n",
    "print(f\"llm true:{sum(trues)/len(trues)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "json_file_path = 'outputs/fever/qwen2-57_fever_output.json'\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "data_fever = [item for item in data if 'cot_answer' in item]\n",
    "data_len = len(data_fever)\n",
    "print(data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Question\", \"True answer\", \"CoT answer\", \"CoT-SC answer\", \"CoT-SC score\", \"CoK answer\"]\n",
    "df_fever = pd.DataFrame(columns = columns)\n",
    "for i in range(data_len):\n",
    "    df_fever.at[i,\"Question\"] = data_fever[i]['question']\n",
    "    df_fever.at[i,\"True answer\"] = data_fever[i]['answer']\n",
    "    df_fever.at[i,\"CoT answer\"] = data_fever[i]['cot_answer'].rstrip('.')\n",
    "    df_fever.at[i,\"CoT-SC answer\"] = data_fever[i]['cot_sc_answer'].rstrip('.')\n",
    "    df_fever.at[i,\"CoT-SC score\"] = data_fever[i]['cot_sc_score']\n",
    "    df_fever.at[i,\"CoK answer\"] = data_fever[i]['final_answer'] if 'final_answer' in data_fever[i].keys() else data_fever[i]['cot_sc_answer'].rstrip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "em:0.62\n",
      "f1:0.62\n"
     ]
    }
   ],
   "source": [
    "ems = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(data_len):\n",
    "    gt = normalize_answer(df_fever.at[i,\"True answer\"])\n",
    "    pred = normalize_answer(df_fever.at[i,\"CoK answer\"])\n",
    "    em = 1 if pred==gt else 0\n",
    "    f1, _, _ = f1_score(pred, gt)\n",
    "    ems.append(em)\n",
    "    f1s.append(f1)\n",
    "\n",
    "print(f\"em:{sum(ems)/len(ems)}\")\n",
    "print(f\"f1:{sum(f1s)/len(f1s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 7,\n",
       " 'verifiable': 'VERIFIABLE',\n",
       " 'label': 'REFUTES',\n",
       " 'claim': 'Kleshas only open the mind.',\n",
       " 'evidence': [[[279111, 273774, 'Kleshas_-LRB-Buddhism-RRB-', 0]],\n",
       "  [[280964, 275356, 'Kleshas_-LRB-Buddhism-RRB-', 0]],\n",
       "  [[280969, 275361, 'Kleshas_-LRB-Buddhism-RRB-', 0]],\n",
       "  [[327302, 314604, 'Kleshas_-LRB-Buddhism-RRB-', 0]],\n",
       "  [[327304, 314605, 'Kleshas_-LRB-Buddhism-RRB-', 0]]],\n",
       " 'question': 'Kleshas only open the mind.',\n",
       " 'answer': 'REFUTES',\n",
       " 's1_domains': ['factual'],\n",
       " 'cot_response': 'First, Kleshas are considered mental states that cloud the mind in Buddhism. Second, Kleshas are typically associated with causing suffering and obstacles, not opening the mind. The answer is REFUTES.',\n",
       " 'cot_answer': 'refutes.',\n",
       " 'cot_sc_score': 1.0,\n",
       " 'cot_sc_response': 'First, Kleshas are considered mental states that cloud the mind in Buddhism. Second, Kleshas are typically associated with causing suffering and obstacles, not opening the mind. The answer is REFUTES.',\n",
       " 'cot_sc_answer': 'refutes.',\n",
       " 'cot_sc_rationales': ['Kleshas are considered mental states that cloud the mind in Buddhism.',\n",
       "  'Kleshas are typically associated with causing suffering and obstacles, not opening the mind.']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fever[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mmlubio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "json_file_path = 'outputs/mmlubio/qwen2-57_mmlubio_output.json'\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "data_mmlubio = [item for item in data if 'cot_answer' in item]\n",
    "data_len = len(data_mmlubio)\n",
    "print(data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'In a population of giraffes, an environmental change occurs that favors individuals that are tallest. As a result, more of the taller individuals are able to obtain nutrients and survive to pass along their genetic information. This is an example of',\n",
       " 'options': ['directional selection.',\n",
       "  'stabilizing selection.',\n",
       "  'sexual selection.',\n",
       "  'disruptive selection.'],\n",
       " 'answer': 'A',\n",
       " 'id': 0,\n",
       " 's1_domains': ['biology'],\n",
       " 'cot_response': \"First, in directional selection, one extreme phenotype becomes more common over time because it provides a survival advantage. Second, stabilizing selection does not fit because it would favor average heights, and sexual selection doesn't apply because there's no mention of mating preferences. Disruptive selection also doesn't fit since it favors extremes rather than one particular extreme. The answer is A.\",\n",
       " 'cot_answer': 'a.',\n",
       " 'cot_sc_score': 1.0,\n",
       " 'cot_sc_response': \"First, in directional selection, one extreme phenotype becomes more common over time because it provides a survival advantage. Second, stabilizing selection does not fit because it would favor average heights, and sexual selection doesn't apply because there's no mention of mating preferences. Disruptive selection also doesn't fit since it favors extremes rather than one particular extreme. The answer is A.\",\n",
       " 'cot_sc_answer': 'a.',\n",
       " 'cot_sc_rationales': ['in directional selection, one extreme phenotype becomes more common over time because it provides a survival advantage.',\n",
       "  \"stabilizing selection does not fit because it would favor average heights, and sexual selection doesn't apply because there's no mention of mating preferences. Disruptive selection also doesn't fit since it favors extremes rather than one particular extreme.\"]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mmlubio[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Question\", \"True answer\", \"CoT answer\", \"CoT-SC answer\", \"CoT-SC score\", \"CoK answer\"]\n",
    "df_mmlubio = pd.DataFrame(columns = columns)\n",
    "for i in range(data_len):\n",
    "    df_mmlubio.at[i,\"Question\"] = data_mmlubio[i]['question']\n",
    "    df_mmlubio.at[i,\"True answer\"] = data_mmlubio[i]['answer']\n",
    "    df_mmlubio.at[i,\"CoT answer\"] = data_mmlubio[i]['cot_answer'].rstrip('.')\n",
    "    df_mmlubio.at[i,\"CoT-SC answer\"] = data_mmlubio[i]['cot_sc_answer'].rstrip('.')\n",
    "    df_mmlubio.at[i,\"CoT-SC score\"] = data_mmlubio[i]['cot_sc_score']\n",
    "    df_mmlubio.at[i,\"CoK answer\"] = data_mmlubio[i]['final_answer'].rstrip('.') if 'final_answer' in data_mmlubio[i].keys() else data_mmlubio[i]['cot_sc_answer'].rstrip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(data_len):\n",
    "    gt = df_mmlubio.at[i,\"True answer\"].lower()\n",
    "    df_mmlubio.at[i, 'CoT correct'] = 1 if df_mmlubio.at[i,\"CoT answer\"].lower()==gt else 0\n",
    "    df_mmlubio.at[i, 'CoT-SC correct'] = 1 if df_mmlubio.at[i,\"CoT answer\"].lower()==gt else 0\n",
    "    df_mmlubio.at[i, 'CoK correct'] = 1 if df_mmlubio.at[i,\"CoK answer\"].lower()==gt else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT acc:0.81\n",
      "CoT-SC acc:0.81\n",
      "CoK acc:0.81\n"
     ]
    }
   ],
   "source": [
    "print(f\"CoT acc:{df_mmlubio['CoT correct'].mean()}\")\n",
    "print(f\"CoT-SC acc:{df_mmlubio['CoT-SC correct'].mean()}\")\n",
    "print(f\"CoK acc:{df_mmlubio['CoK correct'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_mmlubio = df_mmlubio.index[df_fever['CoT-SC score']<0.5].tolist()\n",
    "ids_mmlubio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mmluphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "json_file_path = 'outputs/mmluphy/qwen2-57_mmluphy_output.json'\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "data_mmluphy = [item for item in data if 'cot_answer' in item]\n",
    "data_len = len(data_mmluphy)\n",
    "print(data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'The quantum efficiency of a photon detector is 0.1. If 100 photons are sent into the detector, one after the other, the detector will detect photons',\n",
       " 'options': ['exactly 10 times',\n",
       "  'an average of 10 times, with an rms deviation of about 0.1',\n",
       "  'an average of 10 times, with an rms deviation of about 1',\n",
       "  'an average of 10 times, with an rms deviation of about 3'],\n",
       " 'answer': 'D',\n",
       " 'id': 0,\n",
       " 's1_domains': ['factual', 'physical'],\n",
       " 'cot_response': 'First, the quantum efficiency of the detector is 0.1, meaning that for every photon entering the detector, it has a 10% chance of detecting the photon. Second, since the detections are independent events, the average number of detected photons follows a binomial distribution with parameters n=100 and p=0.1. The expected value (mean) of this distribution is np=10, and the standard deviation (rms deviation) is sqrt(np(1-p))=sqrt(9), which is approximately 3. The answer is D.',\n",
       " 'cot_answer': 'd.',\n",
       " 'cot_sc_score': 1.0,\n",
       " 'cot_sc_response': 'First, the quantum efficiency of the detector is 0.1, meaning that for every photon entering the detector, it has a 10% chance of detecting the photon. Second, since the detections are independent events, the average number of detected photons follows a binomial distribution with parameters n=100 and p=0.1. The expected value (mean) of this distribution is np=10, and the standard deviation (rms deviation) is sqrt(np(1-p))=sqrt(9), which is approximately 3. The answer is D.',\n",
       " 'cot_sc_answer': 'd.',\n",
       " 'cot_sc_rationales': ['the quantum efficiency of the detector is 0.1, meaning that for every photon entering the detector, it has a 10% chance of detecting the photon.',\n",
       "  'since the detections are independent events, the average number of detected photons follows a binomial distribution with parameters n=100 and p=0.1. The expected value (mean) of this distribution is np=10, and the standard deviation (rms deviation) is sqrt(np(1-p))=sqrt(9), which is approximately 3.']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mmluphy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Question\", \"True answer\", \"CoT answer\", \"CoT-SC answer\", \"CoT-SC score\", \"CoK answer\"]\n",
    "df_mmluphy = pd.DataFrame(columns = columns)\n",
    "for i in range(data_len):\n",
    "    df_mmluphy.at[i,\"Question\"] = data_mmluphy[i]['question']\n",
    "    df_mmluphy.at[i,\"True answer\"] = data_mmluphy[i]['answer']\n",
    "    df_mmluphy.at[i,\"CoT answer\"] = data_mmluphy[i]['cot_answer'].rstrip('.')\n",
    "    df_mmluphy.at[i,\"CoT-SC answer\"] = data_mmluphy[i]['cot_sc_answer'].rstrip('.')\n",
    "    df_mmluphy.at[i,\"CoT-SC score\"] = data_mmluphy[i]['cot_sc_score']\n",
    "    df_mmluphy.at[i,\"CoK answer\"] = data_mmluphy[i]['final_answer'].rstrip('.') if 'final_answer' in data_mmluphy[i].keys() else data_mmluphy[i]['cot_sc_answer'].rstrip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(data_len):\n",
    "    gt = df_mmluphy.at[i,\"True answer\"].lower()\n",
    "    df_mmluphy.at[i, 'CoT correct'] = 1 if df_mmluphy.at[i,\"CoT answer\"].lower()==gt else 0\n",
    "    df_mmluphy.at[i, 'CoT-SC correct'] = 1 if df_mmluphy.at[i,\"CoT answer\"].lower()==gt else 0\n",
    "    df_mmluphy.at[i, 'CoK correct'] = 1 if df_mmluphy.at[i,\"CoK answer\"].lower()==gt else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT acc:0.6\n",
      "CoT-SC acc:0.6\n",
      "CoK acc:0.6\n"
     ]
    }
   ],
   "source": [
    "print(f\"CoT acc:{df_mmluphy['CoT correct'].mean()}\")\n",
    "print(f\"CoT-SC acc:{df_mmluphy['CoT-SC correct'].mean()}\")\n",
    "print(f\"CoK acc:{df_mmluphy['CoK correct'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
